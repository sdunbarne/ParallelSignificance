\documentclass[12pt]{article}

\input{../../../../etc/macros}
%\input{../../../../etc/mzlatex_macros}
\input{../../../../etc/pdf_macros}

\bibliographystyle{plain}

\begin{document}

\myheader
\mytitle

\hr

\sectiontitle{Parallel Multiple Chain Significance}

\hr

\usefirefox

\hr

% \visual{Study Tip}{../../../../CommonInformation/Lessons/studytip.png}
% \section*{Study Tip}

% \hr

\visual{Rating}{../../../../CommonInformation/Lessons/rating.png}
\section*{Rating} %one of 
% Everyone: contains no mathematics.
% Student: contains scenes of mild algebra or calculus that may require guidance.
Mathematically Mature: may contain mathematics beyond calculus with proofs.
% Mathematicians Only: prolonged scenes of intense rigor.

\hr

\visual{Section Starter Question}{../../../../CommonInformation/Lessons/question_mark.png}
\section*{Section Starter Question}

\hr

\visual{Key Concepts}{../../../../CommonInformation/Lessons/keyconcepts.png}
\section*{Key Concepts}

\begin{enumerate}
\item
  \begin{theorem}[Parallel \(2 \epsilon\) Test]
    \label{thm:parallelsignificance:twoepstest}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
        \item
            Let \( Y_0, Y_1 \dots, Y_n \) and \( Z_0, Z_1, \dots Z_n \)
            be \( 2 \) independent trajectories of length \( n \) from \(
            X \) from a common starting point \( Y_0 = Z_0 = \sigma_0 \).
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Suppose \( \sigma_0 \sim \pi \).
    \end{enumerate}
    Then for any \( n \)
    \[
        \Prob{v(\sigma_0) \text{is an $\epsilon$-outlier among } v(\sigma_0),
        v(Y_0), v(Y_1) \dots, v(Y_n), v(Z_1), \dots, v(Z_n)} < 2
        \epsilon.
      \]
      
\end{theorem}
  \item \begin{theorem}[Besag-Clifford Test]
    \label{thm:parallelsignificance:bc1}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain on \( \mathcal{X} \)
            with a stationary distribution \( \pi \).
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a label function.
        \item
            Fix a positive integer \( n \).
        \item
            Suppose that \( \sigma_0 \sim \pi \).
        \item
            Integer \( \xi \) is chosen uniformly in \( \set{0,\dots, n}
            \).
        \item
            Let two independent trajectories \( Y_0 , Y_1 \dots \) and \(
            Z_0, Z_1, \dots \) start from \( Y_0 = Z_0 = \sigma_0 \).
    \end{enumerate}
    Then for any \( n \)
    \[
        \Prob{(v(\sigma_0)) \text{ is an $\epsilon$-outlier among } v
        (\sigma_0), v(Y_1), \dots, v(Y_{\xi} ), v(Z_1), \dots, v(Z_{n-\xi}
        )} \le \epsilon.
    \]
\end{theorem}
  \item \begin{theorem}
    \label{thm:parallelsignificance:thm3point1}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
        \item
            Let
            \begin{align*}
                \mathbf{X}^1 &= (X_0^1, X_1^1, \dots, X_k^1 )\\
                &\vdots \\
                \mathbf{X}^m &= (X_0^m, X_1^m, \dots, X_k^m )\\
            \end{align*}
            be \( m \) independent trajectories of length \( n \) from \(
            X \) starting from a common starting point \( X_0^1 =
            X_0^2 = \cdots = X_0^m = \sigma_0 \). 
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Define the random variable \( \mathcal{N} \) to be the
            number of trajectories \( \mathbf{X}^i \) on which \( \sigma_0
            \) is an \( \epsilon \)-outlier.
        \item
            Suppose \( \sigma_0 \) is not an \( (\epsilon, \alpha) \)-outlier.
    \end{enumerate}
    Then
    \[
        \Prob{\mathcal{N} \ge m \sqrt{\frac{2\epsilon}{\alpha}} + r} \le
        \EulerE^{-\min(r^2 \frac{\sqrt{\alpha/(2\epsilon)}}{3m}, \frac{r}
        {3})}.
    \]
\end{theorem}
\item \begin{theorem}
    \label{thm:parallelsignificance:thm6point1}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
          \item
            Let \(n_1, \dots, n_m \) be \( m \) integers independently
            drawn from a geometric distribution.
        \item
            Let
            \begin{align*}
                \mathbf{X}^1 &= (X_0^1, X_1^1, \dots, X_{n_1}^1 )\\
                &\vdots \\
                \mathbf{X}^m &= (X_0^m, X_1^m, \dots, X_{n_m}^m )\\
            \end{align*}
            be \( m \) independent trajectories, each respectively  of length \( n_{i} \), from \(
            X \) starting from a common starting point \( X_0^1 =
            X_0^2 = \cdots = X_0^m = \sigma_0 \). 
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Define the random variable \( \mathcal{N} \) to be the
            number of trajectories \( \mathbf{X}^i \) on which \( \sigma_0
            \) is an \( \epsilon \)-outlier.
        \item
            Suppose \( \sigma_0 \) is not an \( (\epsilon, \alpha) \)-outlier.
    \end{enumerate}
    Then
    \[
        \Prob{\mathcal{N} \ge m \sqrt{\frac{\epsilon}{\alpha}} + r} \le
        \EulerE^{-\min(r^2 \frac{\sqrt{\alpha/(\epsilon)}}{3m}, \frac{r}
        {3})}.
    \]
\end{theorem}
\end{enumerate}

\hr

\visual{Vocabulary}{../../../../CommonInformation/Lessons/vocabulary.png}
\section*{Vocabulary}
\begin{enumerate}
  \item     For a fixed value of \( n \), the state \( \sigma_0 \) is an \defn{\(
    (\epsilon, \alpha) \)-outlier} in \( \mathcal{X} \) if, among all
    states in \( \mathcal{X} \), \( \rho(0; \epsilon, n \given \sigma_{0})
    \) is in the largest \( \alpha \) fraction of the values of \( \rho(0;
    \epsilon, k \given \sigma_{0}) \) over all states \( \sigma \in
    \mathcal{X} \), weighted according to \( \pi \).  In particular,
    being an \( (\epsilon, \alpha) \)-outlier measures the likelihood of
    \( \sigma_0 \) failing the local outlier test, ranked against all
    other states \( \sigma \sim \pi \) of the chain \( X \).
  \item 
\end{enumerate}

\hr

\section*{Notation}
\begin{enumerate}
\item \( n \) -- integer number of steps for a Markov chain
    \item
        \( X \), \( X_0 \), \( X_n \) -- Markov chain, starting state,
        general step of the chain.
    \item
        \( \mathcal{X} \) -- State space of a Markov chain
    \item
        \( v :  \mathcal{X} \to \Reals \) -- value or label function
    \item
        \( \pi \) -- stationary distribution
    \item
        \( x_0 \) -- a given state from the state space
      \item $\sigma_0$ -- starting state from the stationary
        distribution
      \item
            \( Y_0 , Y_1 \dots \) and \(
            Z_0, Z_1, \dots \) -- two independent trajectories  starting from \( Y_0 = Z_0 = \sigma_0 \).

    \item
        \( \alpha_0, \alpha_1, \dots \alpha_n \) -- arbitrary real
        numbers for the definition of outliers
      \item   \( \xi \) -- Integer chosen uniformly in \( \set{0,\dots, k}
            \).

    \item
        \( \epsilon \) -- small real number
    \item
        \[
            \rho(j; \ell, n) = \Prob{X_j \text{ is among- $\ell$
            -smallest from } X_0, \dots, X_n }
        \]
    \item
        \( \rho(j; 0, n \given \sigma) \) -- the probability \( X_j =
        \sigma \) has the unique minimum value in the Markov chain of
        length \( n \) given the Markov passes through \( \sigma \) at
        step \( j \).
      \item     For a fixed value of \( n \), the state \( \sigma_0 \) is an \defn{\(
    (\epsilon, \alpha) \)-outlier} in \( \mathcal{X} \) if, among all
    states in \( \mathcal{X} \), \( \rho(0; \epsilon, n \given \sigma_{0})
    \) is in the largest \( \alpha \) fraction of the values of \( \rho(0;
    \epsilon, k \given \sigma_{0}) \) over all states \( \sigma \in
    \mathcal{X} \), weighted according to \( \pi \).
  \item \( m \) -- number of independent trajectories of length \( n \) from \(
            X \) starting from a common starting point \( X_0^1 =
            X_0^2 = \cdots = X_0^m = \sigma_0 \). 
          \item \( \mathcal{N} \) -- the random variable
            number of trajectories \( \mathbf{X}^i \) on which \( \sigma_0
            \) is an \( \epsilon \)-outlier.
          \item \( p(j; \epsilon,k \given \sigma) \) -- the probability that
\( v(X_j) \) is in the lowest \( \epsilon \) fraction of the values \(
v(X_0), \dots, v(X_n) \) conditioned on the event that \( X_j = \sigma
\).
\item $\mu$ -- mean of a geometric distribution
   \item
     \mathbf{X}^m &= (X_0^m, X_1^m, \dots, X_k^m ) -- one of
     \( m \) independent trajectories of length \( n \) from \(
            X \) starting from a common starting point \( X_0^1 =
            X_0^2 = \cdots = X_0^m = \sigma_0 \). 
          \item $p(\mu; 0, \epsilon \given \sigma)$
 -- 
the probability that $v(X_j)$ is in the lowest $\epsilon$ fraction of the values
$v(X_0), \dots, v(X_n)$, conditioned on the event that $X_0 = σ$ , where
the length $n$ is chosen from a geometric distribution with mean $\mu$
supported on $0,1,2,\dots$;
\end{enumerate}
\visual{Mathematical Ideas}{../../../../CommonInformation/Lessons/mathematicalideas.png}
\section*{Mathematical Ideas}

\subsection*{Introduction}

A 1989 paper of Besag and Clifford
\cite{besag89} described a test related to the \( \sqrt{\epsilon} \)
test based on Proposition~%
\ref{thm:significance:basethm}.

\begin{remark}
    To prove Theorem~%
    \ref{thm:parallelsignificance:thm3point1}, first prove the following, which
    has a quantitative dependence on \( \epsilon \) which is nearly as
    strong as in Theorem~%
    \ref{thm:parallelsignificance:bc1}, while eliminating the need for the
    random choice of \( \xi \).
\end{remark}

To simplify the interpretation of these tests, the approach in this
subsection will show these tests can efficiently separate the measure of
statistical significance from the question of the magnitude of the
effect.  Recall from step~%
\ref{enum:significance:basethm3} in the proof of
Proposition~%
\ref{thm:significance:basethm} the definition for \( 0 \le j \le n \) of
\[
    \rho(j; \ell, n) = \Prob{X_j \text{ is among-$\ell$-smallest
    among } X_0, \dots, X_n }
\] and
\[
  \rho(j; \ell, n \given \sigma) = \Prob{X_j
    \text{ is among-$\ell$-smallest among } X_0, \dots, X_n%
    \given X_j = \sigma }.
\] With a slight change in the parameter notation, define the \( n+1 \)-vector
\[ (\rho(0, \epsilon, n), \rho(1, \epsilon, n), \dots, \rho(1, \epsilon,
  n)) \]
where for each \( i \), \( \rho(i; \epsilon, n) \) is the
probability that in a \( n \)-step stationary trajectory \( X_0, X_1,
\dots X_k \), \( v(X_i) \) is among the smallest \( \epsilon
\)-fraction of indices
in the list \( v(X_0), v(X_1), \dots, v(X_n) \).  Define the probability
\( \rho(0; \epsilon, k \given \sigma_0) \) to be the probability that on a
trajectory \( \sigma_0 = X_0, X_1, \dots, X_n \), \( v(\sigma_0) \) is
among the smallest \( \epsilon \)-fraction of values in the list \( v(X_0),
v(X_1), \dots, v(X_k) \).

\subsection*{The Two Chain Significance Test}

\begin{lemma}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
        \item
            Let \( Y_0, Y_1 \dots, Y_n \) and \( Z_0, Z_1, \dots Z_n \)
            be \( 2 \) independent trajectories of length \( n \) from \(
            X \) from a common starting point \( Y_0 = Z_0 = \sigma_0 \).
        \item
            Suppose \( \sigma_0 \sim \pi \).
    \end{enumerate}
    Then
            \[
                Y_n, Y_{n-1}, \dots, Y_1, \sigma_0, Z_1, Z_2, \dots, Z_n
            \] is a \( \pi \)-stationary trajectory.
\end{lemma}

\begin{proof}
            \begin{enumerate}
                \item
                    Stationarity implies that
                    \[
                        (Z_0, Z_1, \dots, Z_n) \sim (X_n, X_{n+1}, \dots,
                        X_{2n}).
                    \] 
                  \item Stationarity and reversibility imply that
                    \[
                        (Y_n, Y_{n-1}, \dots, Y_1, Y_0) \sim (X_0, X_1,
                        \dots, X_n).
                    \]
                \item
                    The assumption that \( Y_1, Y_2, \dots \) and \( Z_1,
                    Z_2, \dots \) are independent trajectories from \(
                    \sigma_0 \) is equivalent to the condition that
                    \begin{align*}
                        & \Prob{ Z_j = z_j \given Z_{j-1}, \dots, Z_1 =
                        z_1, Z_0 = Y_0=\sigma_0, Y_1=y_1, \dots, Y_n = y_n}
                        \\
                        & \qquad = \Prob{ Z_j = z_j \given Z_{j-1},
                        \dots, Z_1 = z_1, Z_0 = \sigma_0} \\
                        & \qquad = \Prob{ Z_j = z_j \given Z_{j-1}} \\
                        &\qquad = \Prob{ X_{n+j} = z_j \given X_{n+j-1}}.
                    \end{align*}
                \item
                    By induction on \( j \ge 1 \),
                    \[
                        (Y_n, Y_{n-1}, \dots, Y_0=Z_0, Z_1, \dots, Z_j)
                        \sim (X_0, X_1, \dots, X_n, X_{n+1}, \dots, X_{n+j}).
                    \]
                \item
                    In particular, taking \( j = n \)
                    \[
                        (Y_n, Y_{n-1}, \dots, \sigma_0, Z_1, \dots, Z_n)
                        \sim (X_0, X_1, \dots, X_n, X_{n+1}, \dots, X_{2n}).
                    \]
            \end{enumerate}
\end{proof}

Recall, a real number \( \alpha_0 \) is
an \( \epsilon \)-outlier among \( \alpha_0, \dots \alpha_n \) if
\[
    \card{\setof{i \in 0, \dots, n}{ \alpha_i \le \alpha_0 }} \le
    \epsilon(k+1).
\]

\begin{theorem}[Parallel \(2 \epsilon\) Test]
    \label{thm:parallelsignificance:twoepstest}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
        \item
            Let \( Y_0, Y_1 \dots, Y_n \) and \( Z_0, Z_1, \dots Z_n \)
            be \( 2 \) independent trajectories of length \( n \) from \(
            X \) from a common starting point \( Y_0 = Z_0 = \sigma_0 \).
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Suppose \( \sigma_0 \sim \pi \).
    \end{enumerate}
    Then for any \( n \)
    \[
        \Prob{v(\sigma_0) \text{is an $\epsilon$-outlier among } v(\sigma_0),
        v(Y_0), v(Y_1) \dots, v(Y_n), v(Z_1), \dots, v(Z_n)} < 2
        \epsilon.
      \]
      
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item
            Given a Markov chain \( X \) with values \( v:  \mathcal{X}
            \to \Reals \) and stationary distribution \( \pi \), define
            for each \( j, \ell \le n \) the probability \( \rho(j; \ell,
            n) \) that for a \( \pi \)-stationary trajectory \( X_1, X_2,
            \dots, X_n \) the value \( v(X_j) \) is among-\( \ell \)-smallest
            among \( v(X_1), v(X_2), \dots, v(X_n) \). Note that all \(
            \pi \)-stationary trajectories \( X_1, X_2, \dots, X_n \) of
            fixed length \( n \) are all identical in distribution so
            that \( \rho(j; \ell, n) \) is well-defined. See the
            exercises.
        \item
            If the sequence \( X_1, X_2, \dots X_n \) is a \(
            \pi \)-stationary trajectory for \( X \), then \( (X_{n-j},
            \dots X_n, \dots, X_{2n-j}) \), is an identically
            distributed sequence.  Thus the probability that \( v(X_n) \)
            is among-\( \ell \)-smallest among \( v(X_{n-j}), \dots, v(X_n),
            \dots v(X_{2n-j}) \) is equal to \( \rho(j; \ell, n) \).  
          \item In
            particular, since the event
            \[
                [v(X_n) \text{ is among-$\ell$-smallest among } v(X_
                {n-j}), \dots, v(X_n), \dots, v(X_{2n-j})]
            \] is a sub-event of the event
            \[
                [v(X_n) \text{ is among-$\ell$-smallest among } v(X_0),
                \dots, v(X_n), \dots, v(X_{2n})]
            \] for all \( j = 0, \dots n \) it follows that \( \rho(n;
            \ell, 2n) \le \rho(j; \ell, n) \).
        \item
            By linearity of expectation, the sum \( \sum_{\nu=0}^n \rho(
            \nu; \ell, n) \) is the expected number of indices \( j \in
            \set{0, \dots, n} \) such that \( v(X_j) \) is \( \ell \)-small
            among \( v(X_0), \dots, v(X_n) \), so \( \sum_{\nu=0}^n \rho
            (\nu; \ell, n) \le \ell + 1 \).
        \item
            Averaging the left and right sides over \( j \) from \( 0 \)
            to \( n \),
            \[
                \rho(n; \ell, 2n) = \frac{1}{n+1} \sum_{\nu=0}^n \rho(\nu;
                \ell, n) \le \frac{\ell + 1}{n+1} < 2 \cdot \frac{\ell+1}
                {2n+1}.
            \]
        \item
            Using the Lemma completes the proof by noting \( \rho(n; \ell,
            2n) \) is a lower bound on each \( \rho(j; \ell, n) \) and
            then applying the simple inequality
            \begin{equation}
                \label{eq:parallelsignificance:eight} \sum\limits_{j=0}^n \rho(j;
                \ell, n) \le \ell + 1.
            \end{equation}
    \end{enumerate}
\end{proof}

\begin{remark}
    Note that Theorem~%
    \ref{thm:parallelsignificance:twoepstest} is equivalent to the statement that the
    probabilities \( \rho(i; \epsilon, n) \) always satisfy \( \rho(n;
    \epsilon, 2n) < 2\epsilon \). [Why?]  The remaining theoretical question is
    what is the largest possible value of \( p(n; \epsilon, 2n) \) as a
    function of \( \epsilon \).
\end{remark}

\begin{theorem}[Besag-Clifford Test]
    \label{thm:parallelsignificance:bc1}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain on \( \mathcal{X} \)
            with a stationary distribution \( \pi \).
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a label function.
        \item
            Fix a positive integer \( n \).
        \item
            Suppose that \( \sigma_0 \sim \pi \).
        \item
            Integer \( \xi \) is chosen uniformly in \( \set{0,\dots, n}
            \).
        \item
            Let two independent trajectories \( Y_0 , Y_1 \dots \) and \(
            Z_0, Z_1, \dots \) start from \( Y_0 = Z_0 = \sigma_0 \).
    \end{enumerate}
    Then for any \( n \)
    \[
        \Prob{(v(\sigma_0)) \text{ is an $\epsilon$-outlier among } v
        (\sigma_0), v(Y_1), \dots, v(Y_{\xi} ), v(Z_1), \dots, v(Z_{n-\xi}
        )} \le \epsilon.
    \]
\end{theorem}
\index{Besag-Clifford Test}

\begin{remark}
    See Figure~%
    \ref{fig:parallelsignificance:test} for a schematic diagram of the
    Besag-Clifford Test.
\end{remark}

% The following figure is drawn with the primitive LaTeX picture
% environment rather than the more sophisticated Asymptote, since it
% is mostly math environments.  All positions, lengths, and parameters
% are established by experimentation,  I should probably use \shortparallel
% instead of \parallel, but it seemed to not be available.
\begin{figure}
    \centering
    \begin{picture}(100,60)(0,-25)
        \put(0,30){\( Y_0 \rightarrow Y_1 \rightarrow \cdots \rightarrow
        Y_{\xi} \rightarrow \cdots \rightarrow Y_n \)}
        \put(0,15){\( \parallel \)}
        \put(0,0){\( \sigma_0 \)}
        \put(0,-15){\( \parallel \)}
        \put(0,-30){\( Z_0 \rightarrow Z_1 \rightarrow \cdots
        \rightarrow Z_{n - \xi} \rightarrow \cdots \rightarrow Z_n \)}
        \put(90,27){\vector(-1,-1){45}}
    \end{picture}
    \caption{Schematic diagram of the Besag-Clifford Test.  The
    short wide-head arrows are the Markov chain transitions, the long
    narrow-head arrow represents the switch from the $ Y $ trajectory
    to the $ Z $ trajectory.}%
    \label{fig:parallelsignificance:BCtest}
\end{figure}



\begin{proof}[Proof of Theorem~\ref{thm:parallelsignificance:bc1}]
    \begin{enumerate}
        \item
            The proof uses only equation~%
            \eqref{eq:parallelsignificance:eight}
        \item
            Recall from the proof of Theorem~%
            \ref{thm:parallelsignificance:twoepstest} that the \( \rho(j; \ell, n) \)
            are fixed real numbers associated to a stationary Markov
            chain.
        \item
            If \( \ell \) and \( n \) are fixed, and \( \xi \) is chosen
            uniformly at random from \( \set{0, \dots, n} \), then the
            resulting \( \rho(j; \xi, n) \) is a random variable
            uniformly distributed on the set of real numbers \( \set{\rho
            (0; \ell, n), \dots, \rho(n; \ell, n)} \).
        \item
            Now write the probability that \( v(\sigma_0) \) is among-\(
            \ell \)-smallest among \( v(\sigma_0), v(Y_1), \dots, v(Y_{\xi}),
            v(Z_1), \dots, v(Z_ {n-\xi}) \) is
            \[
                \frac{1}{n+1} (\rho(0; \ell, n)+ \rho(1; \ell, n) +
                \cdots + \rho(n; \ell, n) ) \le \frac{\ell + 1}{n + 1}
            \] using equation~\eqref{eq:parallelsignificance:eight}.
        \item
            This observation uses a variant of the Lemma
            that for any \( j \), \( Y_j, \dots, Y_1, \sigma_0, Z_1,
            \dots, Z_{n-j} \) is a \( \pi \)-stationary trajectory.
    \end{enumerate}
\end{proof}

\begin{remark}
    An intuitive interpretation of the theorem is that typical, i.e.\ %
    stationary, states are unlikely to change, as measured by \( v \),
    in a consistent way under two sequences of chain transitions of
    random complementary lengths.
\end{remark}

\begin{remark}
    Notice that \( \epsilon \) would be the correct value of the
    probability if, for example, the Markov chain is simply a collection
    of independent random uniform samples from \( [0,1] \) with the
    identity as the value function.  The values of the \( Y \) and \( Z \)
    trajectories would then be a collection of \( 2n + 1 \) random
    uniform samples from \( [0,1] \) and the probability of \( \Prob{v(\sigma_0)}
    \) is an \( \epsilon \)-outlier would be \( \epsilon \).

    The striking thing about Theorem~%
    \ref{thm:parallelsignificance:bc1} is that it achieves a best-possible
    dependence on the parameter \( \epsilon \).  The sacrifice is in the
    theorem's more complicated intuitive interpretation.

    In applications of these statistical tests to aspects of public
    policy such as the gerrymandering example in , it is desirable to have tests with simple, intuitive
    interpretations.  To enable better significance testing in this
    sphere, one goal of this section is to prove a theorem enabling
    Markov chain significance testing which is intuitively interpretable
    in the sense of Proposition~%
    \ref{thm:significance:basethm} while having linear dependence on \(
    \epsilon \), as in Theorem~%
    \ref{thm:parallelsignificance:bc1}.
\end{remark}

\begin{remark}
    One common feature of the tests based on Proposition~%
    \ref{thm:significance:basethm} and~%
    \ref{thm:parallelsignificance:bc1} is the use of randomness.  In particular,
    the probability space at play in these theorems includes both the
    random choice of \( \sigma_0 \) assumed by the null hypothesis and
    the random steps taken by the Markov chain from \( \sigma_0 \).
    Thus the measures of ``how (globally) unusual'' \( \sigma_0 \) is
    with respect to its performance in the local outlier test and ``how
    sure'' we are that \( \sigma_0 \) is unusual in this respect are
    intertwined in the final significance-value.  In particular, the effect
    size and the statistical significance are not explicitly separated.
\end{remark}

\subsection*{The Multiple Chain Significance Test}

\begin{definition}
    For a fixed value of \( n \), the state \( \sigma_0 \) is an \defn{\(
    (\epsilon, \alpha) \)-outlier} in \( \mathcal{X} \) if, among all
    states in \( \mathcal{X} \), \( \rho(0; \epsilon, n \given \sigma_{0})
    \) is in the largest \( \alpha \) fraction of the values of \( \rho(0;
    \epsilon, k \given \sigma_{0}) \) over all states \( \sigma \in
    \mathcal{X} \), weighted according to \( \pi \).  In particular,
    being an \( (\epsilon, \alpha) \)-outlier measures the likelihood of
    \( \sigma_0 \) failing the local outlier test, ranked against all
    other states \( \sigma \sim \pi \) of the chain \( X \).
\end{definition}

Whether \( \sigma_0 \) is an \( (\epsilon, \alpha) \)-outlier is a
deterministic question about the properties of \( \sigma_0 \), \( X \),
and \( v \).  Thus it is a deterministic measure (defined in terms of
certain probabilities) of the extent to which \( \sigma_0 \) is unusual
(globally, in all of \( \mathcal{X} \)) with respect to its local
fragility in the chain.

\begin{example}
    For example, fix \( n = 10^9 \).  If \( \sigma_0 \) is a \( (10^{-6},
    10^{-5}) \)-outlier in \( X \) and \( \pi \) is the uniform
    distribution, this means that among all states \( \sigma \in
    \mathcal{X} \), \( \sigma_0 \) is more likely than all but a \( 10^{-5}
    \)-fraction of states to have an \( v \)-value in the least \( 10^{-6}
    \)-fraction of values \( v(X_0), v(X_1 ), \dots , v(X_{10^9} ) \).
    The probability space underlying the ``more likely'' claim here just
    concerns the choice of the random trajectory \( X_1, \dots, X_{10^9}
    \) from \( X \).
\end{example}

The following theorem enables asserting statistical significance for the
property of being an \( (\epsilon, \alpha) \)-outlier.  In particular,
while tests based on Proposition~%
\ref{thm:significance:basethm} and Theorem~%
\ref{thm:significance:powerthm} take as their [null hypothesis that \(
\sigma_0 \sim \pi \)], the following
Theorem~\ref{thm:parallelsignificance:thm3point1}
takes as its null hypothesis
merely that \( \sigma_{0} \) is not an \( (\epsilon, \alpha) \)-outlier.

First make an auxiliary technical definition and accompanying lemma.
Define \( p(j; \epsilon,k \given \sigma) \) to be the probability that
\( v(X_j) \) is in the lowest \( \epsilon \) fraction of the values \(
v(X_0), \dots, v(X_n) \) conditioned on the event that \( X_j = \sigma
\).

\begin{lemma}
     If \( \sigma_0 \) is not an \( (\epsilon,\alpha) \)-outlier, then
     \begin{equation}
     \label{eq:parallelsignificance:nine} p(0; \epsilon, n \given
     \sigma_0) \le \sqrt{\frac{2 \epsilon}{\alpha}}.
     \end{equation}
\end{lemma}

\begin{proof}
          \item
            Consider a \( \pi \)-stationary
            trajectory \( X_0, \dots, X_n, \dots, X_{2n} \) and
            condition on the event that \( X_n = \sigma \) for some
            arbitrary \( \sigma \in \mathcal{X} \).  Since \( X \) is
            reversible, we can view this trajectory as two independent
            trajectories, \( X_{n+1}, \dots, X_{2k} \) and \( X_{n-1}, X_
            {k-2}, \dots, X_0 \) both beginning from \( \sigma \).
        \item
            Let \( A \) and \( B \) be the events that \( v(X_n) \) is
            an \( \epsilon \)-outlier among the lists \( v(X_0), \dots,
            v(X_k) \) and \( v(X_n), \dots, v(X_{2k}) \), respectively.
            Then
            \begin{equation}
                \label{eq:parallelsignificance:eleven} (p(0; \epsilon, n \given
                \sigma))^2 = \Prob{ A \intersect B} \le (p(n; \epsilon,
                2n \given \sigma))^2.
            \end{equation}
        \item
            The assumption that the given \( \sigma_0 \in \mathcal{X} \)
            is not an \( (\epsilon, \alpha) \)-outlier means that for a
            random \( \sigma \sim \pi \)
            \begin{equation}
                \label{eq:parallelsignificance:twelve} \Prob{ p(0; \epsilon, n
                \given \sigma) \ge p(0; \epsilon, n \given \sigma_0) }
                \ge \alpha.
            \end{equation}
        \item
            Equations~\eqref{eq:parallelsignificance:eleven} and~\eqref{eq:parallelsignificance:twelve}
            give \( (p(0; \epsilon, n \given \sigma))^2 \le (p(n,
            \epsilon, 2n \given \sigma))^2 \) while Theorem~%
            \ref{thm:parallelsignificance:twoepstest} gives \( (p(0; \epsilon, n
            \given \sigma))^2 \le 2\epsilon \).
        \item
            Taking expectations with respect to a random \( \sigma \sim
            \pi \) gives
            \[
                \Esub{\sigma \sim \pi}{(p(0; \epsilon, n \given \sigma))^2
                } \le \Esub{\sigma \sim \pi}{p(n; \epsilon, 2n \given
                \sigma) } = p(n; \epsilon, 2n) \le 2\epsilon.
            \]
        \item
            On the other hand, with~\eqref{eq:parallelsignificance:twelve}
            \[
                \Esub{\sigma \sim \pi}{(p(0; \epsilon, k \given \sigma))^2
                } \ge \alpha \cdot (p(0; \epsilon, k \given \sigma))^2
            \] so that
            \[
                (p(0; \epsilon, k \given \sigma))^2 \le \frac{2\epsilon}
                {\alpha}.
            \]
\end{proof}

\begin{theorem}
    \label{thm:parallelsignificance:thm3point1}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
        \item
            Let
            \begin{align*}
                \mathbf{X}^1 &= (X_0^1, X_1^1, \dots, X_k^1 )\\
                &\vdots \\
                \mathbf{X}^m &= (X_0^m, X_1^m, \dots, X_k^m )\\
            \end{align*}
            be \( m \) independent trajectories of length \( n \) from \(
            X \) starting from a common starting point \( X_0^1 =
            X_0^2 = \cdots = X_0^m = \sigma_0 \). 
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Define the random variable \( \mathcal{N} \) to be the
            number of trajectories \( \mathbf{X}^i \) on which \( \sigma_0
            \) is an \( \epsilon \)-outlier.
        \item
            Suppose \( \sigma_0 \) is not an \( (\epsilon, \alpha) \)-outlier.
    \end{enumerate}
    Then
    \[
        \Prob{\mathcal{N} \ge m \sqrt{\frac{2\epsilon}{\alpha}} + r} \le
        \EulerE^{-\min(r^2 \frac{\sqrt{\alpha/(2\epsilon)}}{3m}, \frac{r}
        {3})}.
    \]
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:parallelsignificance:thm3point1}]
    \begin{enumerate}
        \item
            For a \( \pi \)-stationary trajectory the
            probability \( p(0; \epsilon, n \given \sigma_0) \) is the
            probability that on a trajectory \( \sigma_0 = X_0, X_1,
            \dots, X_k \), \( v(\sigma_0) \) is among the smallest
            values in the list \( v(X_0), v(X_1), \dots, v(X_n) \).
        \item
            Using the Lemma, recall the random variable \(
            \mathcal{N} \) is the number of trajectories \( X^i \) from \(
            \sigma_0 \) on which \( \sigma_0 \) is observed to be an
            outlier with respect to the valueing \( v \).  The random
            variable \( \mathcal{N} \) is thus the sum of \( m \)
            independent Bernoulli random variables, each of which takes
            value \( 1 \) with probability less than \( \sqrt{\frac{2
            \epsilon}{\alpha}} \).  By Chernoff's bound
            \[
                \Prob{\mathcal{N} \ge (1 + \delta) m \sqrt{\frac{2e}{\alpha}}
                } \le \EulerE^{ -\min (\delta, \delta^2 m \sqrt{\frac{2e}
                {\alpha}}/3)}
            \] establishing the theorem.
    \end{enumerate}
\end{proof}

  \begin{remark}
    Apart from separating measures of statistical significance from the
    quantification of a local outlier, Theorem~%
    \ref{thm:parallelsignificance:thm3point1} connects the intuitive Local
    Outlier Test tied to Proposition~%
    \ref{thm:significance:basethm}, which motivates the definition of \(
    (\epsilon, \alpha) \)-outlier, to the better quantitative dependence
    on \( \epsilon \) in Theorem~%
    \ref{thm:parallelsignificance:bc1} [Not sure what this is trying to say.]
\end{remark}

\begin{remark}
    When \( m \) is a reasonable size for calculation, it may make more
    sense to use the exact binomial tail in place of the Chernoff bound.
    That is the content of the following Corollary.
\end{remark}

\begin{corollary}
    \label{cor:parallelsignificance:cor32}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
        \item
            Let
            \begin{align*}
                \mathbf{X}^1 &= (X_0^1, X_1^1, \dots, X_k^1 )\\
                &\vdots \\
                \mathbf{X}^m &= (X_0^m, X_1^m, \dots, X_k^m )\\
            \end{align*}
            be \( m \) independent trajectories of length \( n \) from \(
            X \) starting from a common starting point \( X_0^1 = X_0^2
            = \cdots = X_0^m = \sigma_0 \).
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Define the random variable \( \mathcal{N} \) to be the
            number of trajectories \( \mathbf{X}^i \) on which \( \sigma_0
            \) is an \( \epsilon \)-outlier.
        \item
            Suppose \( \sigma_0 \) is not an \( (\epsilon, \alpha) \)-outlier.
    \end{enumerate}
    Then
    \[
        \Prob{\sigma_0 \text{ is an $\epsilon$-outlier on all of } \mathbf
        {X}^1, \dots \mathbf{X}^m } \le \left( \frac{2\epsilon}{\alpha}
        \right)^{m/2}.
    \]
\end{corollary}

\begin{example}
    To compare the quantitative performance of Theorem~%
    \ref{thm:parallelsignificance:thm3point1} to Proposition~%
    \ref{thm:significance:basethm} and Theorem~%
    \ref{thm:parallelsignificance:bc1}, consider the case of a state \( \sigma_0
    \) for which a random trajectory \( \sigma_0 = X_0 , X_1 , \dots X_n
    \) is likely (say with some constant probability \( p' \)) to find \(
    \sigma_0 \) an \( \epsilon' \)-outlier.  For Proposition~%
    \ref{thm:significance:basethm}, significance at \( p \approx \sqrt{2\epsilon}
    \) would be obtained, while using Theorem~%
    \ref{thm:parallelsignificance:bc1}, one would hope to obtain significance
    approximately \( O(\epsilon') \).  Applying Theorem~%
    \ref{thm:parallelsignificance:thm3point1}, we would expect to see \( p \)
    around \( m p' \).  In particular, we could demonstrate that \(
    \sigma_0 \) is an \( (\epsilon', \alpha) \)-outlier for \( \alpha =
    \frac{3\epsilon} {(p')^2} \) (a linear relation on \( \epsilon \))
    at a \( p \)-value which can be made arbitrarily small (at an
    exponential rate) as we increase the number of observed trajectories
    \( m \).  [This needs more explanation.]
\end{example}

\begin{corollary}
    Given the hypotheses in Theorem~%
    \ref{thm:parallelsignificance:thm3point1}, then
    \[
        \Prob{\mathcal{N} \ge K} \le \sum\limits_{\nu=K}^m \binom{m}{\nu}
        \left( \sqrt{\frac{2\epsilon}{\alpha}} \right)^{k} \left( 1 -
        \sqrt{\frac{2\epsilon}{\alpha}} \right)^{m-k}.
    \]
\end{corollary}

\subsection*{The Improved Multiple Chain Significance Test}

The following Theorem~\ref{thm:parallelsignificance:thm6point1} is the analog of
Theorem~\ref{thm:parallelsignificance:thm3point1} using an analog of Besag and Clifford’s Theorem~\ref{thm:parallelsignificance:bc1} in
place of Proposition~\ref{thm:significance:basethm} in the proof. This version pays the price of using a
random \( n \) instead of a fixed \( n \) for the notion of an \(
(\epsilon, \alpha) \)-outlier,
but has the advantage that the constant \( 2 \) is eliminated from
the bound. As in \ref{thm:parallelsignificance:thm3point1},
the notion of \( (\epsilon, \alpha) \)-outlier here is still just defined with respect to a single
path, although Theorem~\ref{thm:parallelsignificance:bc1} depends on using two independent
trajectories.

\begin{theorem}
    \label{thm:parallelsignificance:thm6point1}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
          \item
            Let \(n_1, \dots, n_m \) be \( m \) integers independently
            drawn from a geometric distribution.
        \item
            Let
            \begin{align*}
                \mathbf{X}^1 &= (X_0^1, X_1^1, \dots, X_{n_1}^1 )\\
                &\vdots \\
                \mathbf{X}^m &= (X_0^m, X_1^m, \dots, X_{n_m}^m )\\
            \end{align*}
            be \( m \) independent trajectories, each respectively  of length \( n_{i} \), from \(
            X \) starting from a common starting point \( X_0^1 =
            X_0^2 = \cdots = X_0^m = \sigma_0 \). 
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Define the random variable \( \mathcal{N} \) to be the
            number of trajectories \( \mathbf{X}^i \) on which \( \sigma_0
            \) is an \( \epsilon \)-outlier.
        \item
            Suppose \( \sigma_0 \) is not an \( (\epsilon, \alpha) \)-outlier.
    \end{enumerate}
    Then
    \[
        \Prob{\mathcal{N} \ge m \sqrt{\frac{\epsilon}{\alpha}} + r} \le
        \EulerE^{-\min(r^2 \frac{\sqrt{\alpha/(\epsilon)}}{3m}, \frac{r}
        {3})}.
    \]
\end{theorem}

\begin{remark}
  Observe carefully the difference in conclusions of this
  Theorem~\ref{thm:parallelsignificance:thm6point1} from
Theorem~\ref{thm:parallelsignificance:thm3point1} is the absence of
the factor $2$, so Theorem~\ref{thm:parallelsignificance:thm6point1}
an improvement in the probability estimates.  The improvement comes at
the expense of a more complicated set of parallel Markov chains.
\end{remark}

\begin{proof}
  \begin{enumerate}
  \item For a $\pi$-stationary trajectory $X_0, \dots, X_k$
and a real number $\mu$,  define $p(\mu; 0, \epsilon \given \sigma)$
to be
the probability that $v(X_j)$ is in the lowest $\epsilon$ fraction of the values
$v(X_0), \dots, v(X_n)$, conditioned on the event that $X_0 = σ$ , where
the length $n$ is chosen from a geometric distribution with mean $\mu$
supported on $0,1,2,\dots$; that is,
\[
  \Prob{n = t} = \frac{1}{\mu+1}\left( 1 - \frac{1}{\mu+1} \right)^t.
\]
\item{enum:parallelchain:eq15}  It suffices to prove that if $\sigma_0$ is not
an $(\epsilon, \sigma)$-outlier with respect to $n$ drawn from the geometric
distribution with mean $\mu
$, then
\[
  p(\mu; 0, \epsilon \given \sigma) \le
  \sqrt{\frac{\epsilon}{\alpha}}.
\]
\item
  To prove the previous assertion, suppose that $n_1$ and $n_2$ are
  independent
  random variables which are geometrically distributed with mean $\mu$,
and consider a $\pi$-stationary trajectory
$X_0, \dots,  X_n_1, \dots,  X_n_1 +n_2$
of random length $n_1 + n_2$, and condition on the event that
$X_n_1 = \sigma$ for some arbitrary $\sigma \in \mathcal{X}$. Since $X$ is reversible,
we can view this trajectory as two independent trajectories $X_{n_1},
X_{n_1 +1}, \dots,  X_{n_1 +n_2}$
and $X_{n_1} , X_{n_1 −1} , X_{n_1 −2}, \dots,  X_0$ both
beginning from $X_{n_1} = \sigma$ , of random lengths $n_2$ and $n_1$ ,
respectively. 
\item\label{enum:parallelchain:eq16} Letting $A$ and $B$ be the events
that
$v(X_n_1)$ is an $\epsilon$-outlier among the lists $v(X_0 ), \dots, v(X_{n_1})$ and
$v(X_{n_1}), \dots, v(X_{n_1 +n_2})$, respectively,
\[
  p(\mu; 0, \epsilon \given \sigma)^2 = \Prob{v(X_{n_1}) \text{ is an
      $\epsilon$-outlier among}  v(X_0 ), \dots, v(X_{n_1}),
    v(X_{n_1}), \dots, v(X_{n_1 +n_2}) \given X_{n_1} = \sigma}
\]
where, in this last expression, $n_1$ and $n_2$ are random variables.
\item The assumption that the given $\sigma_0 \in \mathcal{X}$ is not
  an $(\epsilon, \alpha)$-outlier gives that for a random $\sigma ∼ \pi$ ,
  \[
    \Prob{p(\mu; 0, \epsilon | \sigma)} \ge \Prob{p(\mu; 0, \epsilon |
      \sigma_{0}} \ge \alpha.
  \]
  \item\label{enum:parallelchain:eq18} 
    Write
    \[
      \alpha \cdot p(\mu; 0, \epsilon | \sigma)^{2} \le \Esub{\sigma ~
        \pi} \le \Prob{v(X_{n_1}) \text{ is an
      $\epsilon$-outlier among}  v(X_0 ), \dots, v(X_{n_1}),
    v(X_{n_1}), \dots, v(X_{n_1 +n_2})
    \]
where the last inequality follows from
step~\ref{enum:parallelchain:eq16}.
\item  Considering the right-hand side of
  step~ref{enum:parallelchain:eq18},
  conditioning on any value for the length
$\ell= n_1 + n_2$ of the trajectory, $k_1$ is uniformly distributed in the
range $0, \dots \ell$. This is ensured by the geometric distribution,
simply because for any $\ell$ and any $x \in (0, \dots, \ell)$,
the probability
\begin{align*}
  &\Prob{ n_1 = x \text{ and } n_2 = \ell-x} \\
  &\quad = \left( \frac{1}{\mu+1}\left( 1 - \frac{1}{\mu+1} \right)^x
    \right) \cdot \left( \frac{1}{\mu+1}\left( 1 - \frac{1}{\mu+1}
    \right)^{\ell-x} \right) \\
  &\quad = \left( \frac{1}{\mu+1} \right)^2 \left( 1 - \frac{1}{\mu+1} \right)^\ell
\end{align*}
is independent of $x$.
\item In particular, conditioning on any particularl value for the
  length $\ell = n_1 + n_2$, the probability $v(X_{n_1})$ is an
  $\epsilon$-outlier on the trajectory is at most $\epsilon$ since
  $X_{k_1}$ is a uniformly randomly chosen element of the trajectory
  $X_0, \dots, X_{n_1 + n_2}$.  This part of the proof is exactly the
  same as the proof of teh Besag-Clifford Theorem.
\item In particular, for the right hand side of the inequality in
  step~\ref{{enum:parallelchain:eq18}} 
    \[
      \alpha \cdot p(\mu; 0, \epsilon | \sigma)^{2}
      \le \Prob{v(X_{n_1}) \text{ is an
      $\epsilon$-outlier among}  v(X_0 ), \dots, v(X_{n_1}),
    v(X_{n_1}), \dots, v(X_{n_1 +n_2})}
      \le \max_i \Prob{v(X_{n_1}) \text{ is an
      $\epsilon$-outlier among}  v(X_0 ), \dots, v(X_{n_1}),
    v(X_{n_1}), \dots, v(X_{n_1 +n_2}) \given n_1 + n_2 = \ell}\le \epsilon.
\]
This establishes the required inequality in
step~\ref{enum:parallelchain:eq15} and completes the proof.
  \end{enumerate}
\end{proof}

\begin{remark}
    When \( m \) is not large, using the exact binomial tail in place of
    the Chernoff bound in~\eqref{eq:parallelsignificance:eleven} may be
    sensible.  This gives the following Corollary.
\end{remark}


\begin{corollary}
    \begin{enumerate}
        \item
            Let \( X \) be a reversible Markov chain.
        \item
            Let
            \begin{align*}
                \mathbf{X}^1 &= (X_0^1, X_1^1, \dots, X_k^1 )\\
                &\vdots \\
                \mathbf{X}^m &= (X_0^m, X_1^m, \dots, X_k^m )\\
            \end{align*}
            be \( m \) independent trajectories of length \( n \) from \(
            X \) starting from a common starting point \( X_0^1 = X_0^2
            = \cdots X_0^m = \sigma_0 \).
        \item
            Let \( v :  \mathcal{X} \to \Reals \) be a value function.
        \item
            Define the random variable \( \mathcal{N} \) to be the
            number of trajectories \( \mathbf{X}^i \) on which \( \sigma_0
            \) is an \( \epsilon \)-outlier.
        \item
            Suppose \( \sigma_0 \) is not an \( (\epsilon, \alpha) \)-outlier.
    \end{enumerate}
    Then
    \[
        \Prob{\mathcal{N} \ge K} \le \sum\limits_ {\nu=K}^m \binom{m}{\nu}
        \left( \sqrt{\frac{\epsilon} {\alpha}} \right)^k \left( 1 - \sqrt{\frac
        {\epsilon}{\alpha}} \right)^ {m-k}.
    \]
\end{corollary}

\subsection*{Multiple Hypothesis Considerations}

Performing several tests, then allowing a significant result on any of
them to be evidence for the same hypothesis, requires multiple testing
correction.  Multiple comparisons%
\index{multiple comparisons}
arise when a statistical analysis involves multiple simultaneous
statistical tests, each of which has a potential to produce a
``discovery'', on the same dataset or dependent datasets.  Specifically,
there is a chance that the hypothesis which is actually true is rejected
by the test, a Type I error.  The error probability increases with
multiple testing.  A stated confidence level generally applies only to
each test considered individually, but often it is desirable to have a
confidence level for the whole family of simultaneous tests.  Failure to
compensate for multiple comparisons can have important consequences,
illustrated by the following simple examples.
\begin{itemize}
    \item
        Suppose the treatment is a new way of teaching writing to
        students, and the control is the standard way of teaching
        writing.  Students in the two groups can be compared in terms of
        grammar, spelling, organization, content, and so on.  These
        writing attributes might plausibly give dependent datasets.  As
        more attributes are compared, it becomes increasingly likely
        that the treatment and control groups will appear to differ on
        at least one attribute due to random sampling error alone.
    \item
        Consider the efficacy of a drug in terms of the reduction of any
        one of a number of disease symptoms.  As more symptoms are
        considered, it becomes increasingly likely that the drug will
        appear to be an improvement over existing drugs in terms of at
        least one symptom, again due to random sampling error alone.
\end{itemize}
In both examples, as the number of comparisons increases, it becomes
more likely that the groups being compared will appear to differ in
terms of at least one attribute.  Confidence that a result will
generalize to independent data should generally be weaker if it is
observed as part of an analysis that involves multiple comparisons,
rather than an analysis that involves only a single comparison.

[The following paragraphs need a lot of explanation and expansion.]

When applying Theorem~%
\ref{thm:parallelsignificance:thm3point1} directly in the current Markov chain
situation, one cannot simply run \( m \) trajectories, observe the list \(
\epsilon_1 , \epsilon_2, \dots, \epsilon_ m \) where each \( \epsilon_i \)
is the minimum \( \epsilon_i \) for which \( \sigma_0 \) is an \(
\epsilon_i \)-outlier on \( X^i \), and then, post-hoc, freely choose
the parameters \( \alpha \) and \( \epsilon \) in Theorem~%
\ref{thm:parallelsignificance:thm3point1} to achieve some desired trade-off
between \( \alpha \) and the significance \( p \).  [This needs some
thought and explanation.] The problem, of course, is that in this case
one is testing multiple hypotheses (infinitely many in fact; one for
each possible pair \( \epsilon \) and \( \alpha \)) which would require
a multiple hypothesis correction.  One way to avoid this problem is to
essentially do a form of cross validation, where a few trajectories are
run for the purposes of selecting suitable \( \epsilon \) and \( \alpha \).
Then discard those few trajectories from the set of trajectories used
for significance.  A simpler approach, however, is to simply set the
parameter \( \epsilon \equiv \epsilon(t) \) as the \( t \)th-smallest
element of the list \( \epsilon_1, \dots, \epsilon_m \) for some fixed
value \( t \).  The case \( t = m \), for example, corresponds to taking
\( \epsilon \) as the maximum value, leading to the application of
Theorem~%
\ref{cor:parallelsignificance:cor32}

The reason this avoids the need for a multiple hypothesis correction is
that hypothesis events are ordered by containment.  In particular, when
we apply this test with some value of \( t \), we will always have \(
\mathcal{N} = t \).  Thus the significance obtained will depend just on
the parameter \( \epsilon(t) \) returned by taking the \( t \)th
smallest \( \epsilon_i \) and on the choice of \( \alpha \) (as opposed
to say, the particular values of the other \( \epsilon_i \)'s which are
not the \( t \)th smallest).  In particular, regardless of how the
values \( \alpha \) and \( p \) trade off, the optimum choice of \(
\alpha \) (for the fixed choice of \( t \)) will depend just on the
value \( \epsilon(t) \).  In particular, take \( \alpha \) as a function
\( \alpha(\epsilon(t) ) \), so applying Theorem~%
\ref{thm:parallelsignificance:thm3point1}, \( \epsilon = \epsilon(t) \),
evaluates the single-parameter infinite family of hypotheses \( H_{\epsilon
(t) \alpha(\epsilon(t)) } \), and multiple hypothesis correction is not
required since the hypotheses are nested.  That is, since
\[
    \epsilon(t) \le \epsilon' (t) \implies H_{\epsilon(t),\alpha(\epsilon
    (t) )} \subseteq H_{\epsilon' (t) ,\alpha(\epsilon(t) )}.
\] Indeed,
\[
    \Prob{ \bigcup_{\epsilon(t) \le \beta } H_{\epsilon(t) ,\alpha(\epsilon
    (t))} } = \Prob{H_{\beta, \alpha(\beta)}},
\] which ensures that when applying Theorem~%
\ref{thm:parallelsignificance:thm3point1} in this scenario, the probability of
returning a \( p \)-value less than or equal to \( p_0 \) for any fixed
value \( p_0 \) will indeed be at most \( p_0 \).

\visual{Section Starter Question}{../../../../CommonInformation/Lessons/question_mark.png}
\section*{Section Ending Answer}

\subsection*{Sources}
This section is adapted from
\cite{doi:10.1080/2330443X.2020.1806763}.  The definition and examples
of multiple hypothesis correction is adapted from the Wikipedia
article on
\link{https://en.wikipedia.org/wiki/Multiple_comparisons_problem}{Multiple
comparisons problem}

\hr

\visual{Algorithms, Scripts, Simulations}{../../../../CommonInformation/Lessons/computer.png}
\section*{Algorithms, Scripts, Simulations}

\subsection*{Algorithm}

\subsection*{Scripts}

\input{ _scripts}

\hr

\visual{Problems to Work}{../../../../CommonInformation/Lessons/solveproblems.png}
\section*{Problems to Work for Understanding}
\renewcommand{\theexerciseseries}{}
\renewcommand{\theexercise}{\arabic{exercise}}

\begin{exercise}
  
\end{exercise}
\begin{solution}
  
\end{solution}
\begin{exercise}
  \begin{enumerate}[label=(\alpha*)]
  \item 
  \end{enumerate}
\end{exercise}
\begin{solution}
  \begin{enumerate}[label=(\alpha*)]
  \item 
  \end{enumerate}
\end{solution}

\hr

\visual{Books}{../../../../CommonInformation/Lessons/books.png}
\section*{Reading Suggestion:}

\bibliography{../../../../CommonInformation/bibliography}

%   \begin{enumerate}
%     \item 
%     \item 
%     \item 
%   \end{enumerate}

\hr

\visual{Links}{../../../../CommonInformation/Lessons/chainlink.png}
\section*{Outside Readings and Links:}
\begin{enumerate}
  \item  
  \item  
  \item  
  \item 
\end{enumerate}

\section*{\solutionsname}
\loadSolutions

\hr

\mydisclaim \myfooter

Last modified:  \flastmod

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
